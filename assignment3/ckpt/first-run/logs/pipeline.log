========================================
GSM8K Self-Improving Training Pipeline
========================================
Run Name: first-run
Initial Model: Qwen3-0.6B
Pipeline Directory: ckpt/first-run
Number of Iterations: 1

Inference Settings:
  Dataset Split: train
  Number of Queries: 2000
  Max Tokens: 512
  Temperature: 1.0
  Top-p: 1
  Top-k: -1
  Number of Rollouts: 8
  Tensor Parallel: 1
  Mode: Zero-shot
  Chat Template: Enabled
  Thinking Mode: Disabled

Training Settings:
  Learning Rate: 2e-5
  Total Batch Size: 128
  Batch Size Per Device: 1
  Gradient Accumulation Steps: 128 (auto-calculated)
  Number of Epochs: 1
  Save Steps: 30
  LoRA Rank: 64
  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)
========================================

========================================
ITERATION 0 / 0
========================================
Model: Qwen3-0.6B

[Step 1/4] Running inference...
Output: ckpt/first-run/iteration_0/inference.jsonl

