{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78718c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 20:30:33 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Few-shot prompt with 8 examples\n",
    "FEW_SHOT_PROMPT = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
    "A: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
    "A: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\n",
    "\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
    "A: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\n",
    "\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
    "A: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\n",
    "\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\n",
    "\n",
    "Q: {question}\n",
    "A:\"\"\"\n",
    "\n",
    "ZERO_SHOT_PROMPT = \"{question}\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.\"\n",
    "\n",
    "def extract_ground_truth(text):\n",
    "    \"\"\"Extract the numerical answer from GSM8K answer text.\n",
    "\n",
    "    Args:\n",
    "        text: Answer text in format \"explanation #### number\"\n",
    "\n",
    "    Returns:\n",
    "        Extracted numerical answer\n",
    "    \"\"\"\n",
    "    return text.split('####')[-1].strip()\n",
    "\n",
    "\n",
    "def load_gsm8k_data(split: str = 'test') -> List[Dict]:\n",
    "    \"\"\"Load the GSM8K dataset.\n",
    "\n",
    "    Args:\n",
    "        split: Dataset split to load ('train' or 'test')\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries with question and answer\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading GSM8K dataset (split: {split})...\")\n",
    "    dataset = load_dataset('gsm8k-local', 'main', split=split, cache_dir=os.path.join(\"../\", '.hf_cache'))\n",
    "    logger.info(f\"Loaded {len(dataset)} examples from GSM8K {split} set\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def format_prompts(\n",
    "    questions: List[str],\n",
    "    use_few_shot: bool = False,\n",
    "    use_chat_template: bool = True,\n",
    "    tokenizer: AutoTokenizer = None,\n",
    "    system_message: str = None,\n",
    "    enable_thinking: bool = False\n",
    ") -> List[str]:\n",
    "    \"\"\"Format GSM8K questions into prompts.\n",
    "\n",
    "    Args:\n",
    "        questions: List of GSM8K questions\n",
    "        use_few_shot: Whether to use few-shot prompting (8 examples)\n",
    "        use_chat_template: Whether to apply chat template\n",
    "        tokenizer: HuggingFace tokenizer for chat template\n",
    "        system_message: Optional system message for chat template\n",
    "        enable_thinking: Whether to enable thinking mode for Qwen3 models (default: False)\n",
    "\n",
    "    Returns:\n",
    "        List of formatted prompts ready for inference\n",
    "    \"\"\"\n",
    "    formatted_prompts = []\n",
    "\n",
    "    # ======================= TODO: Implement this method =========================\n",
    "    # Your task: Format each question into a prompt suitable for the model\n",
    "    #\n",
    "    # Steps:\n",
    "    # 1. For each question:\n",
    "    #    a. Choose the appropriate prompt template based on use_few_shot flag\n",
    "    #       - FEW_SHOT_PROMPT: includes 8 example Q&A pairs (defined above)\n",
    "    #       - ZERO_SHOT_PROMPT: just the question with instruction (defined above)\n",
    "    #    b. Format the template with the question using .format(question=...)\n",
    "    #\n",
    "    # 2. If use_chat_template is True and tokenizer is provided:\n",
    "    #    - Create a messages list in chat format (list of dicts with \"role\" and \"content\")\n",
    "    #    - If system_message is provided, add {\"role\": \"system\", \"content\": system_message}\n",
    "    #    - Add the formatted prompt as {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "    #    - Apply tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    #    - Wrap in try-except to handle models that don't support chat templates\n",
    "    #\n",
    "    # 3. Otherwise: use the formatted prompt directly (without chat template)\n",
    "    #\n",
    "    # Hint: The chat template transforms messages into the model's expected format\n",
    "    # (e.g., \"<|im_start|>user\\n{content}<|im_end|>\" for Qwen models)\n",
    "    # =======================================================================\n",
    "    \n",
    "    for question in questions:\n",
    "        prompt_template = FEW_SHOT_PROMPT if use_few_shot else ZERO_SHOT_PROMPT\n",
    "        formatted_prompt = prompt_template.format(question=question)\n",
    "        \n",
    "        if use_chat_template and tokenizer is not None:\n",
    "            try:\n",
    "                messages = []\n",
    "                if system_message is not None:\n",
    "                    messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "                messages.append({\"role\": \"user\", \"content\": formatted_prompt})\n",
    "                formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=enable_thinking)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error applying chat template: {e}, use raw prompt instead\")\n",
    "        \n",
    "        formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "    return formatted_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f384ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(\n",
    "    model_path: str,\n",
    "    tensor_parallel_size: int = 1,\n",
    "    max_tokens: int = 512,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    top_k: int = -1,\n",
    "    system_message: str = None,\n",
    "    gpu_memory_utilization: float = 0.9,\n",
    "    use_few_shot: bool = False,\n",
    "    use_chat_template: bool = True,\n",
    "    enable_thinking: bool = False,\n",
    "    n_rollouts: int = 1,\n",
    "    split: str = 'test',\n",
    "    n_queries: int = -1,\n",
    ") -> None:\n",
    "    \"\"\"Run VLLM inference on GSM8K dataset.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to the trained model\n",
    "        output_path: Path to save output responses in jsonl format\n",
    "        tensor_parallel_size: Number of GPUs for tensor parallelism\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0.0 for greedy decoding)\n",
    "        top_p: Nucleus sampling parameter\n",
    "        top_k: Top-k sampling parameter (-1 to disable)\n",
    "        batch_size: Batch size for inference (not used in VLLM)\n",
    "        system_message: Optional system message for chat template\n",
    "        gpu_memory_utilization: GPU memory utilization fraction\n",
    "        use_few_shot: Whether to use few-shot prompting (8 examples)\n",
    "        use_chat_template: Whether to apply chat template\n",
    "        enable_thinking: Whether to enable thinking mode for Qwen3 models (default: False)\n",
    "        n_rollouts: Number of rollouts (generations) per question (default: 1)\n",
    "        split: Dataset split to use ('train' or 'test', default: 'test')\n",
    "        n_queries: Number of queries to use from dataset (-1 for all, default: -1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Load GSM8K dataset\n",
    "    dataset = load_gsm8k_data(split=split)\n",
    "    total_queries = len(dataset)\n",
    "\n",
    "    # Limit to n_queries if specified\n",
    "    if n_queries > 0 and n_queries < total_queries:\n",
    "        dataset = dataset.select(range(n_queries))\n",
    "        logger.info(f\"Using subset of {n_queries} queries (from {total_queries} total)\")\n",
    "    else:\n",
    "        logger.info(f\"Using full dataset ({total_queries} queries)\")\n",
    "\n",
    "    questions = [item[\"question\"] for item in dataset]\n",
    "\n",
    "    # Load tokenizer if using chat template\n",
    "    tokenizer = None\n",
    "    if use_chat_template:\n",
    "        logger.info(f\"Loading tokenizer from {model_path}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "    # Format prompts\n",
    "    logger.info(\"Formatting prompts...\")\n",
    "    formatted_prompts = format_prompts(\n",
    "        questions,\n",
    "        use_few_shot=use_few_shot,\n",
    "        use_chat_template=use_chat_template,\n",
    "        tokenizer=tokenizer,\n",
    "        system_message=system_message,\n",
    "        enable_thinking=enable_thinking\n",
    "    )\n",
    "\n",
    "    # Initialize VLLM\n",
    "    logger.info(\"Initializing VLLM engine\")\n",
    "    # ======================= TODO: Implement VLLM inference =========================\n",
    "    # Your task: Use VLLM to generate model outputs for the formatted prompts\n",
    "    #\n",
    "    # Steps:\n",
    "    # 1. Initialize the VLLM LLM engine with appropriate parameters\n",
    "    #    (model path, tensor parallelism, memory utilization, etc.)\n",
    "    #\n",
    "    # 2. Configure sampling parameters:\n",
    "    #    - Consider the difference between greedy decoding (temperature=0.0) and\n",
    "    #      sampling-based generation (temperature>0.0)\n",
    "    #    - When temperature=0.0: deterministic, always picks highest probability token\n",
    "    #    - When temperature>0.0: stochastic, samples from probability distribution\n",
    "    #    - For multiple rollouts (n_rollouts>1), you typically want temperature>0.0\n",
    "    #      to get diverse outputs\n",
    "    #\n",
    "    # 3. Generate outputs using the LLM engine with the formatted prompts\n",
    "    #    Store the results in a variable named \"outputs\"\n",
    "    #\n",
    "    # Hint: Check the VLLM documentation for LLM and SamplingParams classes\n",
    "    # can refer to https://docs.vllm.ai/en/stable/getting_started/quickstart.html\n",
    "    # =======================================================================\n",
    "    DEFAULT_TEMPERATURE = 0.6\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        tensor_parallel_size=tensor_parallel_size,\n",
    "        gpu_memory_utilization=gpu_memory_utilization,\n",
    "    )\n",
    "    sampling_params = SamplingParams(\n",
    "        n=n_rollouts,\n",
    "        temperature= DEFAULT_TEMPERATURE if n_rollouts > 1 and temperature == 0.0 else temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "        \n",
    "    # Generate outputs\n",
    "    outputs = llm.generate(formatted_prompts, sampling_params=sampling_params)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb765aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 20:42:10,042 - INFO - Loading GSM8K dataset (split: test)...\n",
      "Using the latest cached version of the dataset since gsm8k-local couldn't be found on the Hugging Face Hub\n",
      "2025-11-13 20:42:10,328 - WARNING - Using the latest cached version of the dataset since gsm8k-local couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'main' at ../.hf_cache/gsm8k-local/main/0.0.0/5f13071cc147f4f3 (last modified on Thu Nov 13 00:19:00 2025).\n",
      "2025-11-13 20:42:10,330 - WARNING - Found the latest cached dataset configuration 'main' at ../.hf_cache/gsm8k-local/main/0.0.0/5f13071cc147f4f3 (last modified on Thu Nov 13 00:19:00 2025).\n",
      "2025-11-13 20:42:10,334 - INFO - Loaded 1319 examples from GSM8K test set\n",
      "2025-11-13 20:42:10,336 - INFO - Using subset of 20 queries (from 1319 total)\n",
      "2025-11-13 20:42:10,337 - INFO - Loading tokenizer from ../Qwen3-0.6B\n",
      "2025-11-13 20:42:10,600 - INFO - Formatting prompts...\n",
      "2025-11-13 20:42:10,602 - INFO - Initializing VLLM engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 20:42:10 [utils.py:328] non-default args: {'disable_log_stats': True, 'model': '../Qwen3-0.6B'}\n",
      "INFO 11-13 20:42:10 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
      "WARNING 11-13 20:42:10 [__init__.py:2716] Your device 'NVIDIA GeForce RTX 2080 Ti' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
      "WARNING 11-13 20:42:10 [__init__.py:2767] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-13 20:42:10 [__init__.py:1815] Using max model len 40960\n",
      "INFO 11-13 20:42:10 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:11 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:11 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='../Qwen3-0.6B', speculative_config=None, tokenizer='../Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m WARNING 11-13 20:42:12 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m ERROR 11-13 20:42:12 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1113 20:42:23.637198514 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1113 20:42:33.650658973 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1113 20:42:33.652191463 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:33 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m WARNING 11-13 20:42:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:33 [gpu_model_runner.py:2338] Starting to load model ../Qwen3-0.6B...\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:34 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:34 [cuda.py:368] Using FlexAttention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c058312e730940df89e5e8a0b78f7ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:34 [default_loader.py:268] Loading weights took 0.51 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:35 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 0.768070 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:39 [backends.py:539] Using cache directory: /home/timho/.cache/vllm/torch_compile_cache/0bb49a1cc0/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:39 [backends.py:550] Dynamo bytecode transform time: 4.13 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:41 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.797 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:42 [monitor.py:34] torch.compile takes 4.13 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:43 [gpu_worker.py:298] Available KV cache memory: 17.26 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:44 [kv_cache_utils.py:864] GPU KV cache size: 161,616 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:44 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.95x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 24.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:47 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.77 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:47 [gpu_worker.py:391] Free memory on device (20.79/22.0 GiB) on startup. Desired GPU memory utilization is (0.9, 19.8 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.01 GiB for non-torch memory, and 0.77 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=17555316531` to fit into requested memory, or `--kv-cache-memory=18615512064` to fully utilize gpu memory. Current kv cache memory in use is 18537107251 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m INFO 11-13 20:42:47 [core.py:218] init engine (profile, create kv cache, warmup model) took 12.63 seconds\n",
      "INFO 11-13 20:42:48 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 11-13 20:42:48 [__init__.py:36] No IOProcessor plugins requested by the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a836b66bda4346f79c13139874ce381a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15587999c87741e89c0ee76880d6250d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/160 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=397066)\u001b[0;0m CUDAGraph supports dynamic shapes by recording a new graph for each distinct input size. Recording too many CUDAGraphs may lead to extra overhead. We have observed 51 distinct sizes. Please consider the following options for better performance: a) padding inputs to a few fixed number of shapes; or b) set torch._inductor.config.triton.cudagraph_skip_dynamic_graphs=True. Set torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit=None to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "outputs = run_inference(\n",
    "    model_path=\"../Qwen3-0.6B\",\n",
    "    tensor_parallel_size=1,\n",
    "    max_tokens=512,\n",
    "    temperature=0.6,\n",
    "    n_rollouts=8,\n",
    "    n_queries=20,\n",
    "    gpu_memory_utilization=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e6d1790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are given the following information about Janet and her daily activities:\n",
      "\n",
      "- **Janet’s ducks lay 16 eggs per day.**\n",
      "- **She eats 3 eggs for breakfast each morning.**\n",
      "- **She bakes muffins for her friends every day, using 4 eggs per muffin.**\n",
      "- **She sells the remainder at the farmers' market for $2 per fresh duck egg.**\n",
      "\n",
      "---\n",
      "\n",
      "### Step-by-step calculation:\n",
      "\n",
      "#### **1. Total eggs laid per day:**\n",
      "- 16 eggs/day\n",
      "\n",
      "#### **2. Eggs eaten for breakfast:**\n",
      "- 3 eggs\n",
      "\n",
      "#### **3. Eggs eaten for baking muffins:**\n",
      "- 4 eggs/muffin × 1 muffin = 4 eggs\n",
      "\n",
      "#### **4. Total eggs eaten daily:**\n",
      "- 3 (breakfast) + 4 (baking) = **7 eggs/day**\n",
      "\n",
      "#### **5. Eggs remaining for market:**\n",
      "- Total eggs - eggs eaten = $16 - 7 = 9$ eggs/day\n",
      "\n",
      "#### **6. Money made from eggs at the market:**\n",
      "- 9 eggs × $2 per egg = **$18**\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ Final Answer:\n",
      "\n",
      "$$\n",
      "\\boxed{18} \\text{ dollars}\n",
      "$$\n",
      "============\n",
      "We are given the following information about Janet:\n",
      "\n",
      "- **Janet’s ducks lay 16 eggs per day.**\n",
      "- **She eats 3 eggs for breakfast every morning.**\n",
      "- **She bakes muffins for her friends every day with 4 eggs.**\n",
      "- **She sells the remainder at the farmers' market for $2 per fresh duck egg.**\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Calculate the number of eggs Janet lays each day\n",
      "\n",
      "$$\n",
      "16 \\text{ eggs/day}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Subtract the eggs eaten for breakfast\n",
      "\n",
      "$$\n",
      "16 \\text{ eggs/day} - 3 \\text{ eggs} = 13 \\text{ eggs}\n",
      "$$\n",
      "\n",
      "So, she has 13 eggs left from the 16 eggs she lays each day.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 3: Subtract the eggs used for baking muffins\n",
      "\n",
      "$$\n",
      "13 \\text{ eggs} - 4 \\text{ eggs} = 9 \\text{ eggs}\n",
      "$$\n",
      "\n",
      "So, she has 9 eggs left.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 4: Find the earnings from the eggs sold at the farmers' market\n",
      "\n",
      "Each egg is sold for **$2**, so:\n",
      "\n",
      "$$\n",
      "9 \\text{ eggs} \\times 2 \\text{ dollars/egg} = 18 \\text{ dollars}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ Final Answer:\n",
      "\n",
      "$$\n",
      "\\boxed{18} \\text{ dollars}\n",
      "$$\n",
      "============\n",
      "We are told:\n",
      "\n",
      "- Janet’s ducks lay **16 eggs per day**.\n",
      "- She **eats 3 eggs** for breakfast every morning.\n",
      "- She **bakes 4 eggs** for her friends every day.\n",
      "- The **remainder** is sold at the farmers' market for **$2 per fresh egg**.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Calculate total eggs per day\n",
      "\n",
      "- **Eggs per day** = 16 eggs/day  \n",
      "- **Eggs eaten** = 3 eggs  \n",
      "- **Eggs remaining** = 16 - 3 = 13 eggs/day  \n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Calculate eggs sold at the market\n",
      "\n",
      "- **Eggs sold** = 13 eggs  \n",
      "- **Price per egg** = $2  \n",
      "- **Total earnings from market** = 13 × 2 = **$26**\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ Final Answer:\n",
      "\n",
      "$$\n",
      "\\boxed{26} \\text{ dollars}\n",
      "$$\n",
      "============\n",
      "We are given the following information about Janet:\n",
      "\n",
      "- Janet’s ducks lay **16 eggs per day**.\n",
      "- She **eats 3 eggs** for breakfast every morning.\n",
      "- She **bakes 4 eggs** for her friends **every day**.\n",
      "- The **remainder of the eggs** is sold at the market for **$2 per fresh duck egg**.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Calculate total eggs per day\n",
      "\n",
      "Janet lays 16 eggs per day. She eats 3 eggs for breakfast and 4 eggs for baking. That means she uses:\n",
      "\n",
      "$$\n",
      "3 + 4 = 7 \\text{ eggs per day}\n",
      "$$\n",
      "\n",
      "So the **remainder** is:\n",
      "\n",
      "$$\n",
      "16 - 7 = 9 \\text{ eggs per day}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Calculate total earnings from market\n",
      "\n",
      "Each egg is sold at **$2**, and there are 9 eggs per day.\n",
      "\n",
      "$$\n",
      "9 \\times 2 = 18 \\text{ dollars per day}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### Final Answer:\n",
      "\n",
      "$$\n",
      "\\boxed{18} \\text{ dollars}\n",
      "$$\n",
      "============\n",
      "We are given the following information:\n",
      "\n",
      "- **Janet’s ducks lay 16 eggs per day.**\n",
      "- **She eats 3 eggs for breakfast.**\n",
      "- **She bakes muffins for her friends every day with 4 eggs.**\n",
      "- **She sells the remainder at the farmers' market for $2 per fresh egg.**\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Total eggs per day\n",
      "\n",
      "- She lays 16 eggs per day.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Eggs consumed\n",
      "\n",
      "- **Breakfast:** 3 eggs\n",
      "- **Muffins:** 4 eggs\n",
      "\n",
      "So, she eats:  \n",
      "$$ 3 + 4 = 7 \\text{ eggs per day} $$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 3: Eggs remaining after eating\n",
      "\n",
      "$$ 16 - 7 = 9 \\text{ eggs per day} $$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 4: Selling eggs at the market\n",
      "\n",
      "- She sells the remaining 9 eggs for **$2 per egg**\n",
      "\n",
      "$$ 9 \\times 2 = 18 \\text{ dollars per day} $$\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ Final Answer:\n",
      "\n",
      "$$\n",
      "\\boxed{18}\n",
      "$$ dollars per day.\n",
      "============\n",
      "We are given the following details about Janet:\n",
      "\n",
      "- **Janet’s ducks lay 16 eggs per day.**\n",
      "- **She eats 3 eggs for breakfast every morning.**\n",
      "- **She bakes 4 eggs for her friends every day.**\n",
      "- **The remainder is sold at the farmers' market for $2 per fresh duck egg.**\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Calculate total eggs per day\n",
      "\n",
      "She lays 16 eggs per day.\n",
      "\n",
      "She eats 3 eggs for breakfast and 4 eggs for baking.\n",
      "\n",
      "So, she uses:\n",
      "\n",
      "- **3 eggs for breakfast**\n",
      "- **4 eggs for baking**\n",
      "\n",
      "That means she eats **7 eggs per day**.\n",
      "\n",
      "So, the **remainder** is:\n",
      "\n",
      "$$\n",
      "16 \\text{ eggs/day} - 7 \\text{ eggs/day} = 9 \\text{ eggs/day}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Calculate earnings from selling eggs\n",
      "\n",
      "She sells the 9 eggs at the farmers' market for **$2 per egg**.\n",
      "\n",
      "So, the earnings are:\n",
      "\n",
      "$$\n",
      "9 \\text{ eggs/day} \\times \\$2/\\text{egg} = \\$18 \\text{ per day}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ Final Answer:\n",
      "\n",
      "$$\n",
      "\\boxed{\\$18}\n",
      "$$\n",
      "============\n",
      "We are given the following information about Janet's daily activities:\n",
      "\n",
      "- **Janet's ducks lay 16 eggs per day.**\n",
      "- **She eats 3 eggs for breakfast.**\n",
      "- **She bakes 4 eggs for her friends.**\n",
      "- **She sells the remaining eggs at the farmers' market for $2 per egg.**\n",
      "\n",
      "### Step 1: Calculate the number of eggs per day\n",
      "- She lays 16 eggs/day\n",
      "- She eats 3 eggs for breakfast → 3 eggs eaten\n",
      "- She bakes 4 eggs for her friends → 4 eggs baked\n",
      "\n",
      "So, the **remaining eggs** are:\n",
      "$$\n",
      "16 - 3 - 4 = 9\n",
      "$$\n",
      "\n",
      "### Step 2: Calculate total earnings from the market\n",
      "- Each egg is sold for $2 at the market\n",
      "- She sells 9 eggs → total earnings:\n",
      "$$\n",
      "9 \\times 2 = 18\n",
      "$$\n",
      "\n",
      "### Final Answer:\n",
      "$$\n",
      "\\boxed{18}\n",
      "$$\n",
      "\n",
      "Janet makes **$18** every day at the farmers' market.\n",
      "============\n",
      "We are given the following details:\n",
      "\n",
      "- **Janet’s ducks lay 16 eggs per day.**\n",
      "- **She eats 3 eggs for breakfast every morning.**\n",
      "- **She bakes 4 eggs for her friends every day.**\n",
      "- **The remainder is sold at the farmers' market for $2 per egg.**\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Calculate the number of eggs laid each day\n",
      "\n",
      "Janet lays **16 eggs per day**, so the total number of eggs per day is:\n",
      "\n",
      "$$\n",
      "16 \\text{ eggs/day}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Subtract the eggs eaten for breakfast\n",
      "\n",
      "She eats 3 eggs for breakfast every morning. So, the number of eggs left is:\n",
      "\n",
      "$$\n",
      "16 - 3 = 13 \\text{ eggs/day}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 3: Subtract the eggs baked for friends\n",
      "\n",
      "She bakes 4 eggs for her friends every day. So, the number of eggs left is:\n",
      "\n",
      "$$\n",
      "13 - 4 = 9 \\text{ eggs/day}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 4: Calculate the money made from selling eggs at the market\n",
      "\n",
      "She sells the remaining 9 eggs at **$2 per egg**. So the total money made is:\n",
      "\n",
      "$$\n",
      "9 \\times 2 = 18 \\text{ dollars}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ Final Answer:\n",
      "\n",
      "$$\n",
      "\\boxed{18}\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n============\\n\".join([o.text for o in outputs[0].outputs]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
